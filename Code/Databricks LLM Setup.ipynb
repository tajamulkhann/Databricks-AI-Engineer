{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85af021f",
   "metadata": {},
   "source": [
    "# Databricks LLM Setup & Invocation\n",
    "\n",
    "This notebook installs required Databricks + LangChain dependencies, initializes a Databricks-hosted LLM endpoint, and performs a basic inference call using ChatDatabricks.\n",
    "\n",
    "Use Case:\n",
    "- Validate LLM endpoint connectivity\n",
    "- Run prompt-based inference on Databricks Model Serving\n",
    "- Foundation setup for RAG / Agents / GenAI workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939272b1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26538c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for Databricks GenAI & LangChain\n",
    "%pip install -U --quiet \\\n",
    "    databricks-sdk==0.49.0 \\\n",
    "    \"databricks-langchain>=0.4.0\" \\\n",
    "    databricks-agents \\\n",
    "    mlflow[databricks] \\\n",
    "    databricks-vectorsearch==0.55 \\\n",
    "    langchain==0.3.25 \\\n",
    "    langchain_core==0.3.59 \\\n",
    "    bs4==0.0.2 \\\n",
    "    markdownify==0.14.1 \\\n",
    "    pydantic==2.10.1 \\\n",
    "    mlflow \\\n",
    "    openai \\\n",
    "    PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce3042",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Restart Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faba01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python to apply newly installed packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a3dbf",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Databricks LangChain chat interface\n",
    "from databricks_langchain import ChatDatabricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54241660",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Initialize LLM Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Databricks-hosted LLM model\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef449d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Run Inference (Prompt Invocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d82b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a sample prompt to validate LLM response\n",
    "response = chat_model.invoke(\"Who is data fiduciary?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902aee8f",
   "metadata": {},
   "source": [
    "## üß† Why this structure works\n",
    "\n",
    "Readable ‚Üí Clear sections for setup, config, and inference\n",
    "\n",
    "Scalable ‚Üí Easy to extend into RAG / Agents / MLflow tracking\n",
    "\n",
    "Professional ‚Üí Matches real Databricks GenAI notebooks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
